{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This site contains practical, plain-language guides for managing and storing research data in the Lloyd Lab.</p> <p>Who is this for? Anyone in the lab \u2014 no coding experience required.</p> <p>Start with any page from the menu at the top. Suggested starting points:</p> <ul> <li>Action plan: a short checklist to get you set up quickly</li> <li>Project structure: where files should go and why</li> <li>Backups: how we keep data safe</li> </ul> <p>If you get stuck or something is unclear, please contact the Data Steward listed on the Help page of the main README.</p>"},{"location":"action_plan/","title":"Action plan","text":""},{"location":"action_plan/#solution-1-space-recovery","title":"\ud83d\udcbe SOLUTION 1: SPACE RECOVERY","text":""},{"location":"action_plan/#action-1-compress-old-imaging-data-expected-300-500gb-recovery","title":"Action 1: Compress Old Imaging Data (Expected: 300-500GB recovery)","text":"<p>What to compress: - All microscopy images older than 1 year - All folders from past personel - IMC data from completed experiments - Any TIFF files not currently being analyzed</p> <p>How to compress: <pre><code># Quick compression of TIFF files\nfind /path/to/imaging -name \"*.tif\" -mtime +180 -exec python compress_image.py {} \\;\n</code></pre> <code>compress_image.py</code> located in ADD/PATH/TO/FILE</p> <p>Expected results: - 30-50% size reduction with lossless compression - No quality loss, maintains all metadata</p>"},{"location":"action_plan/#action-2-archive-completed-projects","title":"Action 2: Archive Completed Projects","text":"<p>What to archive: - Published papers (raw data already deposited in public repos) - Failed experiments (documented but not analyzed further) - Superseded preliminary data</p> <p>Where to archive: - External hard drive - Institutional archive service (check if available)</p> <p>Archive process: 1. Copy to external storage (with checksum verification) 2. Compress before archiving (ZIP with medium compression) 3. Keep compressed copy on RDS for 30 days 4. Delete from active RDS after verification period</p>"},{"location":"action_plan/#action-3-remove-redundant-files-expected-100-200gb-recovery","title":"Action 3: Remove Redundant Files (Expected: 100-200GB recovery)","text":"<p>Safe to delete: - \u274c Multiple versions of processed data (keep only: raw + final processed + analysis) - \u274c Intermediate analysis files (Jupyter notebook outputs, cache files) - \u274c QC plots and reports (keep summary only, regenerate if needed) - \u274c Duplicate files from failed transfers (use fdupes to find) - \u274c Temporary files in /tmp directories never cleaned</p> <p>How to identify: <pre><code># Find duplicate files\nfdupes -r /path/to/RDS &gt; duplicates.txt\n\n# Find large files not accessed in 180 days\nfind /path/to/RDS -type f -size +100M -atime +180 -ls &gt; large_old_files.txt\n\n# Find jupyter checkpoint files\nfind /path/to/RDS -name \".ipynb_checkpoints\" -type d -exec du -sh {} \\;\n</code></pre></p>"},{"location":"action_plan/#action-4-set-up-storage-monitoring-prevent-future-crises","title":"Action 4: Set Up Storage Monitoring (Prevent future crises)","text":"<p>Automated weekly monitoring: - Check total storage usage - Alert when exceeds 80% capacity (warning) - Alert when exceeds 90% capacity (critical) - Email report to Data Steward and PI</p> <p>See storage_monitor.py script provided</p>"},{"location":"action_plan/#goals","title":"GOALS","text":""},{"location":"action_plan/#month-1-goals","title":"Month 1 Goals","text":"<p>Space Management: - [ ] 500GB+ storage recovered - [ ] Storage usage &lt;70% - [ ] All imaging data &gt;6 months compressed</p> <p>Backup: - [ ] External HDD purchased and connected - [ ] Weekly automated backup running - [ ] Cloud backup account set up</p> <p>Organization: - [ ] All active projects follow directory structure - [ ] All projects have README.md and metadata.yaml - [ ] 0 projects in /NEEDS_ORGANIZATION/</p> <p>Guidelines: - [ ] All lab members trained (100%) - [ ] Quick-reference cards posted - [ ] Templates available in /99_DOCUMENTATION/</p>"},{"location":"action_plan/#month-3-goals","title":"Month 3 Goals","text":"<p>Space Management: - [ ] Storage usage stable at &lt;75% - [ ] No emergency cleanup needed - [ ] Automated compression running monthly</p> <p>Backup: - [ ] 3 successful backup verifications - [ ] Disaster recovery plan tested - [ ] Offsite rotation established</p> <p>Organization: - [ ] 100% compliance with naming conventions - [ ] All new projects use templates - [ ] Automated validation running weekly</p> <p>Guidelines: - [ ] &lt;5 organization questions per month - [ ] All new data saved correctly first time - [ ] Lab members confident with system</p>"},{"location":"action_plan/#month-6-goals","title":"Month 6 Goals","text":"<p>Space Management: - [ ] Storage usage &lt;70% consistently - [ ] Predictable storage growth - [ ] No manual intervention needed</p> <p>Backup: - [ ] 6+ successful monthly verifications - [ ] Zero data loss incidents - [ ] Backup costs within budget</p> <p>Organization: - [ ] Organization is now \"default behavior\" - [ ] Old projects migrated to new structure - [ ] Publication-ready data organization</p> <p>Guidelines: - [ ] System integrated into lab culture - [ ] New members onboard smoothly - [ ] Minimal Data Steward time required</p>"},{"location":"automation_tools/","title":"Automation tools","text":""},{"location":"automation_tools/#automation-tools","title":"\ud83d\udd27 AUTOMATION TOOLS","text":""},{"location":"automation_tools/#tool-1-storage-monitor-prevent-crises","title":"Tool 1: Storage Monitor (Prevent crises)","text":"<p>Purpose: Alert before running out of space</p> <p>Usage: <pre><code># Run weekly (automated via cron)\npython storage_monitor.py /RDS_Lab_Storage --threshold-warning 80 --threshold-critical 90\n</code></pre></p> <p>Output: - Email alert when &gt;80% full (warning) - Email alert when &gt;90% full (critical) - Weekly report of storage trends</p> <p>See storage_monitor.py script (provided earlier)</p>"},{"location":"automation_tools/#tool-2-imaging-compressor-free-up-space","title":"Tool 2: Imaging Compressor (Free up space)","text":"<p>Purpose: Compress old imaging data automatically</p> <p>Usage: <pre><code># Compress all TIFF images older than 90 days\npython compress_imaging.py --input /RDS/00_ACTIVE_PROJECTS --days-old 90 --format ome-tiff\n</code></pre></p> <p>Expected results: - 30-50% size reduction - No quality loss - Maintains all metadata - Automatically verifies integrity</p>"},{"location":"automation_tools/#tool-3-organization-validator-enforce-structure","title":"Tool 3: Organization Validator (Enforce structure)","text":"<p>Purpose: Check projects follow required structure</p> <p>Usage: <pre><code># Check all active projects weekly\npython validate_organization.py /RDS/00_ACTIVE_PROJECTS\n</code></pre></p> <p>Checks: - README.md exists - metadata.yaml exists and valid - File naming conventions followed - Required subdirectories present</p> <p>Output: - List of non-compliant projects - Specific issues for each project - Email report to Data Steward</p>"},{"location":"automation_tools/#tool-4-backup-verifier-ensure-backups-work","title":"Tool 4: Backup Verifier (Ensure backups work)","text":"<p>Purpose: Verify backup integrity automatically</p> <p>Usage: <pre><code># Verify all checksums in backup directory\npython verify_backup.py /backup/external_hdd\n</code></pre></p> <p>Checks: - All files have checksums - Checksums match actual file hashes - No corrupted files in backup</p> <p>Output: - Number of files verified - List of any errors found - Email report if errors detected</p>"},{"location":"backups/","title":"Backups","text":""},{"location":"backups/#solution-2-backup-redundancy-strategy","title":"\ud83d\udd12 SOLUTION 2: BACKUP &amp; REDUNDANCY STRATEGY","text":""},{"location":"backups/#the-3-2-1-rule-industry-standard","title":"The 3-2-1 Rule (Industry Standard)","text":"<p>3 copies of data: - 1 primary (RDS) - 2 backups (external HDD + OneDrive)</p> <p>2 different media types: - RDS (RAID storage) - External hard drive (geographically separated) - OneDrive (different technology) </p> <p>1 offsite copy: - External HDD at PI's office</p>"},{"location":"backups/#recommended-setup-for-your-lab","title":"Recommended Setup for Your Lab","text":"<p>What gets backed up where:</p> <p>Tier 1 - Critical (3 copies): - Raw data from active projects - Metadata files - Unpublished analysis results \u2192 RDS + External HDD + Cloud</p> <p>Tier 2 - Important (2 copies): - Processed data - Analysis code - Published data \u2192 RDS + External HDD</p> <p>Tier 3 - Reproducible (1 copy): - Analysis outputs - Figures for presentations - Exploratory notebooks \u2192 RDS only</p>"},{"location":"backups/#automated-backup-schedule","title":"Automated Backup Schedule","text":"<p>Daily (5 minutes, automated): - Metadata files (metadata.yaml, README.md) - Lab notebooks - Code repositories (git push)</p> <p>Weekly (2 hours, automated): - New raw data from past 7 days - Updated processed datasets</p> <p>Monthly (manual, 30 minutes): - Full backup verification - Rotate external HDD offsite - Cloud sync critical data</p>"},{"location":"backups/#backup-scripts-automated","title":"Backup Scripts (Automated)","text":"<p>Script 1: Daily Metadata Backup <pre><code>#!/bin/bash\n# Backs up all metadata files daily\nrsync -av --include='metadata.yaml' --include='README.md' --exclude='*'   /RDS/00_ACTIVE_PROJECTS/ /backup/daily_metadata/\n</code></pre></p> <p>Script 2: Weekly Raw Data Backup <pre><code>#!/bin/bash\n# Backs up new raw data weekly\nfind /RDS/00_ACTIVE_PROJECTS -name \"01_raw_data\" -type d -mtime -7 |   xargs -I {} rsync -av {} /backup/weekly_rawdata/\n</code></pre></p> <p>Script 3: Monthly Cloud Sync <pre><code>#!/bin/bash\n# Syncs critical data to cloud monthly\nrclone sync /RDS/00_ACTIVE_PROJECTS/*/01_raw_data/   backblaze:lab-backup/raw_data/   --progress\n</code></pre></p> <p>See detailed scripts in automation section</p>"},{"location":"backups/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":"<p>Scenario 1: RDS System Failure 1. Use external HDD as temporary working storage 2. Continue critical experiments 3. Restore to new RDS when available</p> <p>Scenario 2: Ransomware Attack 1. Disconnect RDS immediately 2. Do NOT connect external HDD (may be infected) 3. Restore from cloud backup to new clean system</p> <p>Scenario 3: Accidental Deletion 1. Check RDS trash/snapshots (if available) 2. Restore from last weekly external HDD backup 3. If &gt;1 week old, restore from monthly cloud backup</p> <p>Recovery Time: - Critical data: &lt;4 hours (from external HDD) - Important data: &lt;24 hours - Reproducible data: &lt;1 week (regenerate)</p>"},{"location":"key_challenges/","title":"Key challenges","text":""},{"location":"key_challenges/#key-challenges-solutions","title":"\ud83c\udfaf KEY CHALLENGES &amp; SOLUTIONS","text":""},{"location":"key_challenges/#challenge-1-running-out-of-space","title":"Challenge 1: Running Out of Space \u26a0\ufe0f","text":"<p>Current state: 2TB RDS storage, generating large imaging data biweekly  </p> <p>Immediate Actions: 1. \u2705 Compress old imaging data (300-500GB recovery) 2. \u2705 Archive completed projects externally (200-300GB recovery) 3. \u2705 Remove redundant processed files (100-200GB recovery) 4. \u2705 Set up automated storage monitoring</p>"},{"location":"key_challenges/#challenge-2-backupredundancy","title":"Challenge 2: Backup/Redundancy \ud83d\udcbe","text":"<p>Current state: No backup strategy Target: 3-2-1 backup rule (3 copies, 2 media, 1 offsite)  </p> <p>Immediate Actions: 1. \u2705 Identify key storage locations (OneDrive, RDS personal, RDS projects, external hardrives) 2. \u2705 Create hierarchy of data storage for key datasets (confocal, flow cytometry, IMC, RNAseq, spatial transcriptomics)  4. \u2705 Create disaster recovery protocol</p>"},{"location":"key_challenges/#challenge-3-lack-of-organization","title":"Challenge 3: Lack of Organization \ud83d\udcc1","text":"<p>Current state: Unclear where files should go Target: Standardized directory structure with enforcement  </p> <p>Immediate Actions: 1. \u2705 Deploy mandatory directory template 2. \u2705 Create \"Where does this go?\" decision tree 3. \u2705 Implement monthly organization compliance checks 4. \u2705 Migrate existing data to new structure</p>"},{"location":"key_challenges/#challenge-4-clear-structureguidelines","title":"Challenge 4: Clear Structure/Guidelines \ud83d\udccb","text":"<p>Current state: No documented protocols Target: Simple, enforceable one-page guides  </p> <p>Immediate Actions: 1. \u2705 Create quick-reference cards (starting project, saving data, monthly hygiene) 2. \u2705 Establish mandatory naming conventions 3. \u2705 Set up automated validation tools 4. \u2705 Train all lab members (1-hour session)</p>"},{"location":"key_challenges/#implementation-checklist","title":"\ud83d\udea8 IMPLEMENTATION CHECKLIST","text":""},{"location":"key_challenges/#assessment-quick-wins","title":"Assessment &amp; Quick Wins","text":"<ul> <li> Run storage audit (identify space hogs)</li> <li> Audit external hard drive for backup space</li> <li> Identify 3-5 completed projects (or personel) to archive</li> <li> List all imaging data &gt;1 year old</li> </ul>"},{"location":"key_challenges/#space-recovery","title":"Space Recovery","text":"<ul> <li> Compress 5 largest old imaging datasets</li> <li> Move completed projects to archive staging</li> <li> Delete temporary analysis files from all projects</li> </ul>"},{"location":"key_challenges/#backup-setup","title":"Backup Setup","text":"<ul> <li> Connect external HDD</li> <li> Install backup automation scripts</li> <li> Run first full backup</li> <li> Verify backup integrity</li> </ul>"},{"location":"key_challenges/#organization-rollout","title":"Organization Rollout","text":"<ul> <li> Create directory template on RDS</li> <li> Generate README and metadata templates</li> <li> Print and post quick-reference cards</li> <li> Schedule lab meeting for training</li> </ul>"},{"location":"new_project_template/","title":"GUIDELINES","text":""},{"location":"new_project_template/#one-page-quick-reference-cards","title":"One-Page Quick Reference Cards","text":""},{"location":"new_project_template/#card-1-starting-a-new-project","title":"CARD 1: Starting a New Project","text":"<p>\u2611 NEW PROJECT CHECKLIST (15 minutes)</p> <ol> <li> Create folder: <code>Project_YYYYMM_ShortName</code></li> <li> Copy template README.md from <code>/99_DOCUMENTATION/Templates/</code></li> <li> Fill in README.md (project title, PI, description, data types)</li> <li> Copy template metadata.yaml</li> <li> Fill in metadata.yaml (at minimum: project_id, pi, start_date)</li> <li> Create subdirectories:</li> <li>01_raw_data/ (with data type folders)</li> <li>02_processed/</li> <li>03_analysis/</li> <li>04_figures/</li> <li> Add project to lab inventory: <code>/99_DOCUMENTATION/project_inventory.xlsx</code></li> <li> Notify Data Steward via email</li> </ol> <p>Time investment: 15 minutes Prevents: Hours of future confusion and data loss</p>"},{"location":"new_project_template/#card-2-saving-new-data","title":"CARD 2: Saving New Data","text":"<p>\u2611 NEW DATA CHECKLIST (5 minutes per dataset)</p> <ol> <li> Save to correct location:</li> <li>Imaging \u2192 <code>01_raw_data/imaging/</code></li> <li>IMC \u2192 <code>01_raw_data/imc/</code></li> <li>scRNA-seq \u2192 <code>01_raw_data/scrnaseq/</code></li> <li> <p>Spatial \u2192 <code>01_raw_data/spatial/</code></p> </li> <li> <p> Use naming convention:    <code>YYYYMMDD_DataType_Sample_Condition.ext</code></p> </li> <li> <p> Generate checksum:    <pre><code>sha256sum myfile.tif &gt; myfile.tif.sha256\n</code></pre></p> </li> <li> <p> Update metadata.yaml:</p> </li> <li>Add new dataset entry</li> <li> <p>Record date, sample info, experimental conditions</p> </li> <li> <p> Backup will run automatically (weekly)</p> </li> </ol> <p>Time investment: 5 minutes Prevents: Data corruption, lost samples, cannot reproduce results</p>"},{"location":"new_project_template/#card-3-monthly-data-hygiene","title":"CARD 3: Monthly Data Hygiene","text":"<p>\u2611 MONTHLY CHECKLIST (1 hour per month)</p> <p>Week 1: - [ ] Run storage monitor: <code>python storage_monitor.py /RDS</code> - [ ] Review storage report - [ ] If &gt;80% full, proceed to emergency cleanup</p> <p>Week 2: - [ ] Compress imaging data &gt;3 months old - [ ] Expected recovery: 100-200GB - [ ] Run: <code>python compress_old_imaging.py --days 90</code></p> <p>Week 3: - [ ] Move projects &gt;6 months inactive to <code>01_ARCHIVED_PROJECTS/</code> - [ ] Delete temporary analysis files (cache/, *.tmp) - [ ] Find: <code>find /RDS -name \"cache\" -type d -exec rm -rf {} \\;</code></p> <p>Week 4: - [ ] Verify backup integrity (spot check 5 random files) - [ ] Rotate external HDD (swap onsite/offsite) - [ ] Update lab data inventory</p> <p>Time investment: 1 hour per month Prevents: Storage crises, data loss, backup failures</p>"},{"location":"new_project_template/#mandatory-templates","title":"Mandatory Templates","text":""},{"location":"new_project_template/#readmemd-template-required-for-all-projects","title":"README.md Template (Required for all projects)","text":"<pre><code># Project: [Title]\n\n## Project Info\n- **ID:** Project_YYYYMM_Name\n- **PI:** [Name]\n- **Lead:** [Name]\n- **Start:** YYYY-MM-DD\n- **Status:** Active/Archived/Published\n\n## Description\n[2-3 sentences describing research question]\n\n## Data Types\n- [ ] Imaging\n- [ ] IMC\n- [ ] scRNA-seq\n- [ ] Spatial transcriptomics\n\n## Key Files\n- Raw data: `01_raw_data/`\n- Processed: `02_processed/`\n- Analysis: `03_analysis/`\n- Figures: `04_figures/`\n\n## Notes\n[Any important notes about this project]\n\n## Publications\n[List any papers from this project]\n</code></pre>"},{"location":"new_project_template/#metadatayaml-template-required-for-all-projects","title":"metadata.yaml Template (Required for all projects)","text":"<pre><code># Project Metadata\nproject_id: \"Project_YYYYMM_Name\"\npi_name: \"Dr. Name\"\npi_email: \"pi@university.edu\"\nlead_researcher: \"Researcher Name\"\nstart_date: \"YYYY-MM-DD\"\nstatus: \"active\"  # active, archived, published\nfunding_source: \"Grant XYZ\"\n\n# Data Summary\ndata_types:\n  - imaging\n  - imc\n  - scrnaseq\n  - spatial\n\ntotal_size_gb: 0  # Update periodically\n\n# Raw Data Inventory\nraw_data:\n  - dataset_id: \"20240115_Imaging_Experiment1\"\n    date_acquired: \"2024-01-15\"\n    data_type: \"imaging\"\n    modality: \"confocal\"\n    sample_type: \"mouse_brain\"\n    size_gb: 50\n    location: \"01_raw_data/imaging/20240115_Experiment1/\"\n    notes: \"Initial pilot experiment\"\n\n# Processing History\nprocessing_log:\n  - date: \"2024-01-20\"\n    action: \"Segmentation and quantification\"\n    software: \"CellProfiler 4.2\"\n    output: \"02_processed/imaging/20240120_segmented/\"\n\n# Publications\npublications: []\n\n# Last Updated\nlast_updated: \"YYYY-MM-DD\"\nupdated_by: \"Name\"\n</code></pre>"},{"location":"project_structure/","title":"STANDARDIZED ORGANIZATION","text":""},{"location":"project_structure/#mandatory-directory-structure","title":"Mandatory Directory Structure","text":"<pre><code>/RDS_Lab_Storage/\n\u2502\n\u251c\u2500\u2500 00_ACTIVE_PROJECTS/          # Current work (&lt;6 months)\n\u2502   \u2514\u2500\u2500 Project_YYYYMM_Name/\n\u2502       \u251c\u2500\u2500 README.md            # REQUIRED\n\u2502       \u251c\u2500\u2500 metadata.yaml        # REQUIRED\n\u2502       \u251c\u2500\u2500 01_raw_data/\n\u2502       \u2502   \u251c\u2500\u2500 imaging/\n\u2502       \u2502   \u251c\u2500\u2500 imc/\n\u2502       \u2502   \u251c\u2500\u2500 scrnaseq/\n\u2502       \u2502   \u2514\u2500\u2500 spatial/\n\u2502       \u251c\u2500\u2500 02_processed/\n\u2502       \u251c\u2500\u2500 03_analysis/\n\u2502       \u251c\u2500\u2500 04_figures/\n\u2502       \u2514\u2500\u2500 05_manuscript/\n\u2502\n\u251c\u2500\u2500 01_ARCHIVED_PROJECTS/        # Completed (&gt;6 months)\n\u2502   \u2514\u2500\u2500 [Same structure]\n\u2502\n\u251c\u2500\u2500 02_SHARED_DATASETS/          # Reference data\n\u2502   \u251c\u2500\u2500 Public_scRNAseq/\n\u2502   \u2514\u2500\u2500 Reference_Genomes/\n\u2502\n\u251c\u2500\u2500 03_ANALYSIS_PIPELINES/       # Reusable code\n\u2502   \u251c\u2500\u2500 imaging_pipeline/\n\u2502   \u251c\u2500\u2500 imc_pipeline/\n\u2502   \u2514\u2500\u2500 scrna_pipeline/\n\u2502\n\u251c\u2500\u2500 04_ARCHIVE_READY/            # Staging for external archival\n\u2502\n\u2514\u2500\u2500 99_DOCUMENTATION/            # Templates &amp; guides\n    \u251c\u2500\u2500 Templates/\n    \u2514\u2500\u2500 Protocols/\n</code></pre>"},{"location":"project_structure/#file-naming-convention-mandatory","title":"File Naming Convention (Mandatory)","text":"<p>Format: <pre><code>YYYYMMDD_DataType_Sample_Condition.extension\n</code></pre></p> <p>Examples:</p> <p>Imaging: <pre><code>20240115_Confocal_MouseBrain_Section1_DAPI.tif\n20240115_Confocal_MouseBrain_Section1_GFP.tif\n</code></pre></p> <p>IMC: <pre><code>20240115_IMC_Tumor_Patient01_Core1.mcd\n20240115_IMC_Panel_37markers.csv\n</code></pre></p> <p>scRNA-seq: <pre><code>20240115_scRNAseq_PBMC_Donor01_counts.h5ad\n20240115_scRNAseq_PBMC_Donor01_filtered.h5ad\n</code></pre></p> <p>Analysis outputs: <pre><code>20240115_DiffExp_TumorVsNormal_DESeq2.csv\n20240115_Clustering_Res08_UMAP.pdf\n</code></pre></p>"},{"location":"project_structure/#decision-tree-where-does-this-file-go","title":"Decision Tree: \"Where Does This File Go?\"","text":"<p>START: I have a new file</p> <p>\u2753 Is it RAW data from instrument?    \u2192 YES: <code>01_raw_data/[data_type]/</code>    \u2192 NO: Continue</p> <p>\u2753 Is it PROCESSED/QC'd data?    \u2192 YES: <code>02_processed/</code>    \u2192 NO: Continue</p> <p>\u2753 Is it ANALYSIS code or results?    \u2192 YES: <code>03_analysis/</code>    \u2192 NO: Continue</p> <p>\u2753 Is it a FIGURE for publication?    \u2192 YES: <code>04_figures/</code>    \u2192 NO: Continue</p> <p>\u2753 Is it SHARED reference data?    \u2192 YES: <code>02_SHARED_DATASETS/</code>    \u2192 NO: Ask Data Steward</p>"},{"location":"project_structure/#enforcement-mechanisms","title":"Enforcement Mechanisms","text":"<p>Automated weekly check (Mondays): <pre><code># Check all projects have required files\nfind /RDS/00_ACTIVE_PROJECTS -type d -maxdepth 1 | while read project; do\n  if [ ! -f \"$project/README.md\" ]; then\n    echo \"MISSING README: $project\" &gt;&gt; /var/log/compliance_issues.txt\n    mv \"$project\" /RDS/NEEDS_ORGANIZATION/\n  fi\n  if [ ! -f \"$project/metadata.yaml\" ]; then\n    echo \"MISSING METADATA: $project\" &gt;&gt; /var/log/compliance_issues.txt\n    mv \"$project\" /RDS/NEEDS_ORGANIZATION/\n  fi\ndone\n\n# Email compliance report to Data Steward\nmail -s \"Weekly Organization Compliance Report\" data-steward@uni.edu &lt; /var/log/compliance_issues.txt\n</code></pre></p> <p>Manual enforcement: - Projects in <code>/NEEDS_ORGANIZATION/</code> cannot be worked on - Must add missing files to restore access - Data Steward reviews and approves restoration</p>"},{"location":"storage_by_data_type/","title":"Storage by data type","text":""},{"location":"storage_by_data_type/#storage-optimization-by-data-type","title":"Storage Optimization by Data Type","text":""},{"location":"storage_by_data_type/#imaging-data","title":"Imaging Data","text":"<p>Problem: Generates most data biweekly, quickly fills storage</p> <p>Solutions: 1. Immediate compression (30-50% reduction)    - Convert TIFF \u2192 OME-TIFF with LZW compression    - No quality loss, maintains metadata</p> <ol> <li>Tiered storage by age:</li> <li>&lt;3 months: Keep original on RDS (active work)</li> <li>3-12 months: Compressed OME-TIFF on RDS</li> <li> <p>12 months: Archive to external HDD, keep only thumbnails on RDS</p> </li> <li> <p>Delete intermediate files:</p> </li> <li>Keep: Raw images + final processed images</li> <li>Delete: All intermediate processing steps (can regenerate)</li> </ol> <p>Expected space savings: 40-60% of imaging storage</p>"},{"location":"storage_by_data_type/#imc-data","title":"IMC Data","text":"<p>Problem: Large MCD files + extracted TIFF duplicates</p> <p>Solutions: 1. Keep only MCD files (raw data) 2. Delete extracted TIFFs after analysis complete 3. Compress MCD files in HDF5 format (30% reduction)</p>"},{"location":"storage_by_data_type/#scrna-seq","title":"scRNA-seq","text":"<p>Solutions: 1. Archive raw FASTQ files to external storage 2. Keep only processed count matrices on RDS 3. Use Parquet format for count matrices (50-70% reduction)</p>"},{"location":"storage_by_data_type/#spatial-transcriptomics","title":"Spatial Transcriptomics","text":"<p>Solutions: 1. Keep original files off Xenium machine on external HDD 2. Keep compressed back up on RDS project folder 3. Delete intermediate analysis files after project completion</p>"}]}